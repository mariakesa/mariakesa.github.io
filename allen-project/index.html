<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="histogram1.js"></script>
    <script src="embedding_illustration.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/pyodide/v0.20.0/full/pyodide.js"></script>
    <script src="test.js"></script>
</head>
<body>
    <img src="img/Chica uurib asja, Päikesekiireke tatsab ülakorrusel.gif" height="300">
    <h1>Directions in Vision Transformer feature space modulate single neuron activity in mouse visual cortex</h1>

    <h3>Introduction</h3>

    <p>
        Transformers are a class of deep learning models that employ an algorithmic innovation called the attention mechanism
        to alter representations of sequence elements according to other elements that appear in the sequence. 
        Originally invented for natural language applications 
        in machine translation, they were ported to the computer vision domain in 2021(1). The original Vision Transformer is an encoder
        that models an image as a sequence of non-overlapping flattened patches. 
    </p>

    <p> Universal image representation from which information can be decoded</p>

    <p>
        Nowadays, these motivations are synergistic—experimental neuroscientists are providing new clues and constraints about the algorithmic solution at work in the brain, and computational neuroscientists seek to integrate these clues to produce hypotheses (a.k.a. algorithms) that can be experimentally distinguished.
        Surya Ganguli, Ila Fiete papers on learning rules
    </p>

    <script type="module">
      // Your pre-generated data URL (replace with the actual URL)
      const preGeneratedDataUrl = 'https://raw.githubusercontent.com/mariakesa/mariakesa.github.io/main/allen-project/img/rastermap_test.png';

      // Function to run Python code using Pyodide
      async function runPyodideCode() {
          // Load Pyodide
          let pyodide = await loadPyodide();

          // Load matplotlib-pyodide backend
          await pyodide.loadPackage("matplotlib");

          // Set the matplotlib backend to wasm_backend
          pyodide.runPython(`
              import matplotlib
              matplotlib.use("module://matplotlib.backends.backend_wasm")
          `);

          // Python code to display the image using Matplotlib
          const pythonCode = `
              import base64
              import matplotlib.pyplot as plt
              import numpy as np
              from io import BytesIO

              data_url = '${preGeneratedDataUrl}'
              img_data = base64.b64decode(data_url.split(',')[1])
              image = plt.imread(BytesIO(img_data))

              plt.imshow(image)
              plt.axis('off')
              plt.show()
          `;

          // Run Python code using Pyodide
          await pyodide.runPythonAsync(pythonCode);
      }

      // Attach the function to the global window object
      window.runPyodideCode = runPyodideCode;

      // Call the function when the script is loaded
      runPyodideCode();
  </script>
    <!--<div id="show_embeddings">
      <svg id="embedding_plot" width="800" height="600"></svg>
    </div>-->
    <table>
        <tr>
          <th>Model</th>
          <th>Training data</th>
          <th>Pre-training  method</th>
        </tr>
        <tr>
          <td>Original Vision Transformer, vit-base-patch32-384</td>
          <td>ImageNet-21k, JFT-300M, finetuned on ImageNet-1k</td>
          <td>Supervised classification</td>
        </tr>
        <tr>
          <td>DINO-vitb8</td>
          <td>ImageNet-1k</td>
          <td>Self-supervised</td>
        </tr>
        <tr>
            <td>CLIP-vit-base-patch32</td>
            <td>400 million image-text pairs collected from the internet</td>
            <td>Supervised- predict which caption goes with which image</td>
        </tr>
    </table>

    <div id="container" class="svg-container">
      <svg id="my_histogram" width="800" height="600"></svg>
    </div>


    <script>
      // Call the main function to construct the histogram and append it to the SVG element
      main();
      random_im();
    </script>
  

    <h3>References</h3>
    <p>Radford, A., et al. Learning Transferable Visual Models From Natural Language Supervision.
        https://arxiv.org/pdf/2103.00020.pdf
    </p>
    <p>DiCarlo, J., et al. How Does the Brain Solve Visual Object Recognition? Neuron</p>
    <p>https://www.biorxiv.org/content/10.1101/2021.06.16.448730v5</p>
</body>
</html>
