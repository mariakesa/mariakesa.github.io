<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Self-Attention Tutorial</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="split_patches.js" defer></script>
    <script src="linear_QKV.js" defer></script>
    <script src="attention_matrix.js" defer></script>
    <script src="attention_matrix2.js" defer></script>
</head>
<body>

    <h1>Transformer Self-Attention</h1>

    <div>Introduction</div>

    <div>
        The Vision Transformer operates on images that are broken into patches.
        Self-attention compresses relationships between patches into contextual embeddings.
    </div>


    <div>Splitting into patches</div>

    <div id="split-patches-container"></div>


    <div>Linear embedding into QKV</div>

    <div>
        Image patches are mapped into a space, where their similarity to other patches 
        determines how much influence they have on each other. This step happens by flattening
        image patches and passing them through three matrices (QKV, which are specified by learned parameters) to
        produce Query, Key, and Value vectors.
    </div>

    <div id="linear-QKV-container"></div> 

    <div>Attention matrix computation based on QK</div>

    <div id="attention-matrix-container"></div>

    <div id="attention-matrix-container-2"></div>


    <div>Softmax based averaging of V</div>
    
</body>
</html>